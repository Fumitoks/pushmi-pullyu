
  | Name       | Type       | Params
------------------------------------------
0 | activation | LeakyReLU  | 0     
1 | layer1     | Sequential | 640   
2 | layer2     | Sequential | 73.9 K
3 | layer3     | Sequential | 295 K 
4 | layer4     | Sequential | 1.2 M 
5 | fc1        | Linear     | 262 K 
6 | fc2        | Linear     | 5.1 K 
------------------------------------------
1.8 M     Trainable params
0         Non-trainable params
1.8 M     Total params
/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional_tensor.py:876: UserWarning: Argument fill/fillcolor is not supported for Tensor input. Fill value is zero
  warnings.warn("Argument fill/fillcolor is not supported for Tensor input. Fill value is zero")
/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Trying to log at a previous step. Use `commit=False` when logging metrics manually.
  warnings.warn(*args, **kwargs)
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 49 < 15000; dropping {'train_loss': -0.0020331060513854027, 'train_loss1': 0.0009221278014592826, 'train_loss2': 0.004456914495676756, 'train_loss3': 0.0015016807010397315}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 99 < 30000; dropping {'train_loss': -0.0035659391433000565, 'train_loss1': 0.0081167072057724, 'train_loss2': 0.023461297154426575, 'train_loss3': 0.011778650805354118}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 149 < 45000; dropping {'train_loss': -0.07749192416667938, 'train_loss1': 0.003591360291466117, 'train_loss2': 0.09991782158613205, 'train_loss3': 0.018834535032510757}.
Epoch 0, global step 183: val_loss reached -0.12660 (best -0.12660), saving model to "/content/gdrive/My Drive/PUSHMI/saved_models/distance_300_5e-05_0.0/model.ckpt" as top 1
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 183 < 55000; dropping {'val_loss': -0.12660275399684906, 'val_acc': 0.2898, 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 199 < 59800; dropping {'train_loss': -0.1781773567199707, 'train_loss1': 0.015362005680799484, 'train_loss2': 0.23365509510040283, 'train_loss3': 0.040115732699632645}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 249 < 74800; dropping {'train_loss': -0.1271478682756424, 'train_loss1': 0.03768746554851532, 'train_loss2': 0.29247263073921204, 'train_loss3': 0.12763731181621552}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 299 < 89800; dropping {'train_loss': -0.15980999171733856, 'train_loss1': 0.047021009027957916, 'train_loss2': 0.33991485834121704, 'train_loss3': 0.13308386504650116}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 349 < 104800; dropping {'train_loss': -0.19359077513217926, 'train_loss1': 0.05051707476377487, 'train_loss2': 0.37858492136001587, 'train_loss3': 0.13447706401348114}.
Epoch 1, global step 367: val_loss reached -0.21162 (best -0.21162), saving model to "/content/gdrive/My Drive/PUSHMI/saved_models/distance_300_5e-05_0.0/model.ckpt" as top 1
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 367 < 110000; dropping {'val_loss': -0.21161530911922455, 'val_acc': 0.2708, 'epoch': 1}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 399 < 119600; dropping {'train_loss': -0.2575390338897705, 'train_loss1': 0.0189715176820755, 'train_loss2': 0.3979870080947876, 'train_loss3': 0.12147645652294159}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 449 < 134600; dropping {'train_loss': -0.2869669198989868, 'train_loss1': 0.053410302847623825, 'train_loss2': 0.41628947854042053, 'train_loss3': 0.07591225206851959}.
/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  warnings.warn(*args, **kwargs)
Epoch 2, step 487: val_loss was not in top 1

Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount("/content/gdrive", force_remount=True).
/content/gdrive/My Drive/PUSHMI/pushmi_pullyu
# github.com:22 SSH-2.0-babeld-9d6fcaa2
Hi Fumitoks! You've successfully authenticated, but GitHub does not provide shell access.
The autoreload extension is already loaded. To reload it, use:
  %reload_ext autoreload
Mon Jan 11 20:01:58 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |
| N/A   43C    P0    39W / 300W |   1423MiB / 16130MiB |      0%      Default |
|                               |                      |                 ERR! |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Checkpoint directory /content/gdrive/My Drive/PUSHMI/saved_models/distance_300_5e-05_0.0 exists and is not empty.
  warnings.warn(*args, **kwargs)
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name       | Type       | Params
------------------------------------------
0 | activation | LeakyReLU  | 0     
1 | layer1     | Sequential | 640   
2 | layer2     | Sequential | 73.9 K
3 | layer3     | Sequential | 295 K 
4 | layer4     | Sequential | 1.2 M 
5 | fc1        | Linear     | 262 K 
6 | fc2        | Linear     | 5.1 K 
------------------------------------------
1.8 M     Trainable params
0         Non-trainable params
1.8 M     Total params
/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional_tensor.py:876: UserWarning: Argument fill/fillcolor is not supported for Tensor input. Fill value is zero
  warnings.warn("Argument fill/fillcolor is not supported for Tensor input. Fill value is zero")
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 0 < 146000; dropping {'val_loss': tensor(0.0020, device='cuda:0'), 'val_acc': 0.11333333333333333}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 300 < 146000; dropping {'train_loss': tensor(0.0017, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 600 < 146000; dropping {'train_loss': tensor(0.0014, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 900 < 146000; dropping {'train_loss': tensor(0.0015, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 1200 < 146000; dropping {'train_loss': tensor(0.0012, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 1500 < 146000; dropping {'train_loss': tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 1800 < 146000; dropping {'train_loss': tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 2100 < 146000; dropping {'train_loss': tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 2400 < 146000; dropping {'train_loss': tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 2700 < 146000; dropping {'train_loss': tensor(-0.0007, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 3000 < 146000; dropping {'train_loss': tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 3300 < 146000; dropping {'train_loss': tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 3600 < 146000; dropping {'train_loss': tensor(8.3610e-05, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 3900 < 146000; dropping {'train_loss': tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 4200 < 146000; dropping {'train_loss': tensor(-4.7347e-05, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 4500 < 146000; dropping {'train_loss': tensor(-0.0010, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 4800 < 146000; dropping {'train_loss': tensor(-0.0010, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 5100 < 146000; dropping {'train_loss': tensor(-0.0008, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 5400 < 146000; dropping {'train_loss': tensor(-0.0004, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 5700 < 146000; dropping {'train_loss': tensor(-0.0011, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 6000 < 146000; dropping {'train_loss': tensor(-0.0015, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 6300 < 146000; dropping {'train_loss': tensor(-0.0017, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 6600 < 146000; dropping {'train_loss': tensor(-0.0009, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 6900 < 146000; dropping {'train_loss': tensor(-0.0009, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 7200 < 146000; dropping {'train_loss': tensor(-0.0019, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 7500 < 146000; dropping {'train_loss': tensor(-0.0015, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 7800 < 146000; dropping {'train_loss': tensor(-0.0023, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 8100 < 146000; dropping {'train_loss': tensor(-0.0014, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 8400 < 146000; dropping {'train_loss': tensor(-0.0025, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 8700 < 146000; dropping {'train_loss': tensor(-0.0031, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 9000 < 146000; dropping {'train_loss': tensor(-0.0029, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 9300 < 146000; dropping {'train_loss': tensor(-0.0018, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 9600 < 146000; dropping {'train_loss': tensor(-0.0030, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 9900 < 146000; dropping {'train_loss': tensor(-0.0042, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 10200 < 146000; dropping {'train_loss': tensor(-0.0026, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 10500 < 146000; dropping {'train_loss': tensor(-0.0035, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 10800 < 146000; dropping {'train_loss': tensor(-0.0017, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 11100 < 146000; dropping {'train_loss': tensor(-0.0041, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 11400 < 146000; dropping {'train_loss': tensor(-0.0036, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 11700 < 146000; dropping {'train_loss': tensor(-0.0021, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 12000 < 146000; dropping {'train_loss': tensor(-0.0045, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 12300 < 146000; dropping {'train_loss': tensor(-0.0043, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 12600 < 146000; dropping {'train_loss': tensor(-0.0045, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 12900 < 146000; dropping {'train_loss': tensor(-0.0037, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 13200 < 146000; dropping {'train_loss': tensor(-0.0068, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 13500 < 146000; dropping {'train_loss': tensor(-0.0035, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 13800 < 146000; dropping {'train_loss': tensor(-0.0045, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 14100 < 146000; dropping {'train_loss': tensor(-0.0058, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 14400 < 146000; dropping {'train_loss': tensor(-0.0057, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 14700 < 146000; dropping {'train_loss': tensor(-0.0061, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 15000 < 146000; dropping {'train_loss': tensor(-0.0062, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 15300 < 146049; dropping {'train_loss': tensor(-0.0049, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 15600 < 146049; dropping {'train_loss': tensor(-0.0047, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 15900 < 146049; dropping {'train_loss': tensor(-0.0060, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 16200 < 146049; dropping {'train_loss': tensor(-0.0051, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 16500 < 146049; dropping {'train_loss': tensor(-0.0060, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 16800 < 146049; dropping {'train_loss': tensor(-0.0093, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 17100 < 146049; dropping {'train_loss': tensor(-0.0049, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 17400 < 146049; dropping {'train_loss': tensor(-0.0096, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 17700 < 146049; dropping {'train_loss': tensor(-0.0049, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 18000 < 146049; dropping {'train_loss': tensor(-0.0079, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 18300 < 146049; dropping {'train_loss': tensor(-0.0079, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 18600 < 146049; dropping {'train_loss': tensor(-0.0149, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 18900 < 146049; dropping {'train_loss': tensor(-0.0109, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 19200 < 146049; dropping {'train_loss': tensor(-0.0055, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 19500 < 146049; dropping {'train_loss': tensor(-0.0122, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 19800 < 146049; dropping {'train_loss': tensor(-0.0056, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 20100 < 146049; dropping {'train_loss': tensor(-0.0165, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 20400 < 146049; dropping {'train_loss': tensor(-0.0124, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 20700 < 146049; dropping {'train_loss': tensor(-0.0141, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 21000 < 146049; dropping {'train_loss': tensor(-0.0138, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 21300 < 146049; dropping {'train_loss': tensor(-0.0097, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 21600 < 146049; dropping {'train_loss': tensor(-0.0109, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 21900 < 146049; dropping {'train_loss': tensor(-0.0105, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 22200 < 146049; dropping {'train_loss': tensor(-0.0167, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 22500 < 146049; dropping {'train_loss': tensor(-0.0165, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 22800 < 146049; dropping {'train_loss': tensor(-0.0106, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 23100 < 146049; dropping {'train_loss': tensor(-0.0098, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 23400 < 146049; dropping {'train_loss': tensor(-0.0185, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 23700 < 146049; dropping {'train_loss': tensor(-0.0110, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 24000 < 146049; dropping {'train_loss': tensor(-0.0135, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 24300 < 146049; dropping {'train_loss': tensor(-0.0163, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 24600 < 146049; dropping {'train_loss': tensor(-0.0121, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 24900 < 146049; dropping {'train_loss': tensor(-0.0202, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 25200 < 146049; dropping {'train_loss': tensor(-0.0141, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 25500 < 146049; dropping {'train_loss': tensor(-0.0136, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 25800 < 146049; dropping {'train_loss': tensor(-0.0186, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 26100 < 146049; dropping {'train_loss': tensor(-0.0109, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 26400 < 146049; dropping {'train_loss': tensor(-0.0261, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 26700 < 146049; dropping {'train_loss': tensor(-0.0317, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 27000 < 146049; dropping {'train_loss': tensor(-0.0222, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 27300 < 146049; dropping {'train_loss': tensor(-0.0205, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 27600 < 146049; dropping {'train_loss': tensor(-0.0210, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 27900 < 146049; dropping {'train_loss': tensor(-0.0263, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 28200 < 146049; dropping {'train_loss': tensor(-0.0281, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 28500 < 146049; dropping {'train_loss': tensor(-0.0423, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 28800 < 146049; dropping {'train_loss': tensor(-0.0177, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 29100 < 146049; dropping {'train_loss': tensor(-0.0372, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 29400 < 146049; dropping {'train_loss': tensor(-0.0254, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 29700 < 146049; dropping {'train_loss': tensor(-0.0207, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 30000 < 146049; dropping {'train_loss': tensor(-0.0147, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 30300 < 146099; dropping {'train_loss': tensor(-0.0141, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 30600 < 146099; dropping {'train_loss': tensor(-0.0096, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 30900 < 146099; dropping {'train_loss': tensor(-0.0329, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 31200 < 146099; dropping {'train_loss': tensor(-0.0279, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 31500 < 146099; dropping {'train_loss': tensor(-0.0305, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 31800 < 146099; dropping {'train_loss': tensor(-0.0305, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 32100 < 146099; dropping {'train_loss': tensor(-0.0347, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 32400 < 146099; dropping {'train_loss': tensor(-0.0403, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 32700 < 146099; dropping {'train_loss': tensor(-0.0282, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 33000 < 146099; dropping {'train_loss': tensor(-0.0544, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 33300 < 146099; dropping {'train_loss': tensor(-0.0462, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
[34m[1mwandb[0m: [33mWARNING[0m Step must only increase in log calls.  Step 33600 < 146099; dropping {'train_loss': tensor(-0.0469, device='cuda:0', grad_fn=<AddBackward0>), 'epoch': 0}.
/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  warnings.warn(*args, **kwargs)
